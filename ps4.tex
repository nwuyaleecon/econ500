%Jennifer Pan, August 2011

\documentclass[10pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.

\usepackage{amsmath}
\usepackage{amssymb}
	% packages that allow mathematical formatting
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{graphicx}
	% package that allows you to include graphics

\usepackage{setspace}
	% package that allows you to change spacing

\onehalfspacing
	% text become 1.5 spaced

\usepackage{fullpage}
	% package that specifies normal margins


\begin{document}
	% line of code telling latex that your document is beginning


\title{ECON500: Problem Set 4}

\author{Nicholas Wu}

\date{Fall 2020}
	% Note: when you omit this command, the current dateis automatically included

\maketitle
	% tells latex to follow your header (e.g., title, author) commands.


\section*{Part 1}
\paragraph{(1.1)}
We require
\begin{itemize}
\item $x(p,w)$ to be homogeneous of degree 0 in $p,w$
\item $p \cdot x(p,w) = w$
\item The Slutsky matrix, whose entries are: \[ S_{lk} = \frac{\partial x_l}{\partial p_k} + \frac{\partial x_l}{\partial w}x_k \]
is symmetric and negative definite.
\end{itemize}
\paragraph{(1.2)}
\begin{itemize}
\item We first check that these are homogeneous of degree 0 in $(p,w)$. Indeed:
\[ x_1(p,w) = \frac{w}{p_1 + \alpha p_2}= \frac{\lambda w}{\lambda p_1 + \alpha \lambda p_2} = x_1(\lambda p, \lambda w) \]
\[ x_2(p,w) = \frac{w}{p_1 + \alpha p_2}= \frac{\lambda w}{\lambda p_1 + \alpha \lambda p_2} = x_2(\lambda p, \lambda w) \]
Now, we check that Walras's law is satisfied:
\[ p_1x_1 + p_2x_2 = \frac{p_1 w}{p_1 + \alpha p_2} + \frac{p_2 w}{p_1 + \alpha p_2} = \frac{w(p_1 + p_2)}{(p_1 + \alpha p_2)}  = w\]
For this, we need
\[  \frac{p_1 + p_2}{p_1 + \alpha p_2} = 1 \]
\[ \alpha = 1 \]
Lastly, we check that for this value of $\alpha$, the Slutsky matrix has the desired properties. The Slutsky matrix is
\[ S =
\begin{bmatrix}
-\frac{w}{(p_1 + p_2)^2} + \frac{1}{(p_1 + p_2)}\frac{w}{p_1 + p_2} & -\frac{w}{(p_1 + p_2)^2} + \frac{1}{(p_1 + p_2)}\frac{w}{p_1 + p_2}  \\
-\frac{w}{(p_1 + p_2)^2} + \frac{1}{(p_1 + p_2)}\frac{w}{p_1 + p_2} & -\frac{w}{(p_1 + p_2)^2} + \frac{1}{(p_1 + p_2)}\frac{w}{p_1 + p_2}
\end{bmatrix}
= \begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
 \]
which is clearly negative semidefinite and symmetric. Hence, these are demand functions iff $\alpha = 1$ (fails Walras's law at least if $\alpha \neq 1$).
\item The third demand is implied by Walras' law:
\[ x_3 = \frac{w - p_1x_1 - p_2x_2}{p_3}  \]
We first check for homogeneity:
\[ x_1(\lambda p,\lambda w) = \alpha_1 + \lambda(\beta_1 p_1 + \gamma_1 p_2) = \alpha_1 + \beta_1 p_1 + \gamma_1 p_2  \]
In order for this equality to hold for $\lambda \neq 1$, we need $\beta_1 = 0, \gamma_1 = 0$. Similarly, for
\[ x_2(\lambda p,\lambda w) = \alpha_2 + \lambda(\beta_2p_1 + \gamma_2 p_2) = \alpha_2 + \beta_2 p_1 + \gamma_2 p_2  \]
We also require $\beta_2, \gamma_2 = 0$. Finally, we check homogeneity on the last good:
\[ x_3(\lambda p, \lambda w) = \frac{\lambda w - \lambda p_1 x_1 - \lambda p_2 x_2}{\lambda p_3} = x_3(p, w) \]
We note by construction, these demands satisfy Walras's law. Finally, we compute the Slutsky matrix. Note that from the parameters we found, $x_1$ and $x_2$ are constant demands, so
\[ x_3( p, w) = \frac{w - p_1\alpha_1 - p_2\alpha_2}{p_3} \]
So the Slutsky matrix is:
\[\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
\frac{-\alpha_1}{p_3} + \frac{1}{p_3}\alpha_1 & \frac{-\alpha_2}{p_3} +\frac{1}{p_3}\alpha_2 & -\frac{w - p_1\alpha_1 - p_2\alpha_2}{p_3^2} + \frac{1}{p_3}\frac{w - p_1\alpha_1 - p_2\alpha_2}{p_3}
\end{bmatrix}
= \begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}  \]
And once again this is clearly negative semidefinite and symmetric.

\item Once again, we define $x_3$ such that Walras' law is satisfied. For homogeneity of degree 0, we require:
\[ x_1(\lambda p,\lambda w) = \alpha_1 + \lambda(\beta_1 p_1 + \gamma_1 p_2) + \lambda^2\delta_1p_1p_2 = \alpha_1 + \beta_1 p_1 + \gamma_1 p_2 + \delta_1 p_1p_2 \]
For this to hold, we must have
\[ \lambda(\beta_1 p_1 + \gamma_1 p_2) + \lambda^2\delta_1p_1p_2 = \beta_1 p_1 + \gamma_1 p_2 + \delta_1 p_1p_2 \]
\[ (\lambda-1)(\beta_1 p_1 + \gamma_1 p_2) + (\lambda^2-1)\delta_1p_1p_2 = 0  \]
\[ \beta_1 p_1 + \gamma_1 p_2 + (\lambda+1)\delta_1p_1p_2 = 0  \]
The only way for this to be satisfied at all $\lambda, p_1, p_2$, is to have $\beta_1, \gamma_1, \delta_1 = 0$. The symmetric argument happens for $x_2$: we require $\beta_2, \gamma_2, \delta_2 = 0$. Thus, this is the exact same case as the previous part. We know Walras' law holds by construction of $x_3$. The demands $x_1$, $x_2$, and $x_3$ are the same as the previous part, and hence the Slutsky matrix is the same, so it is symmetric and negative semidefinite.
\end{itemize}
\paragraph{(1.3)}
We will prove that in the case of $2$ goods, we can always derive the expenditure function.

To show symmetry, it suffices to check the two off-diagonal entries. By Walras' law,
\[ p_1 x_1 + p_2 x_2 = w \]
\[ x_2 = \frac{w - p_1 x_1}{p_2} \]
Then we have
\[ S_{21} = \frac{\partial x_2}{\partial p_1} + \frac{\partial x_2}{\partial w} x_1 \]
\[ = - \left( \frac{x_1}{p_2} + \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1} \right) + \left( \frac{1}{p_2} - \frac{p_1}{p_2}\frac{\partial x_1}{ \partial w}\right) x_1 \]
\[ = - \left( \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1} \right) - \left( \frac{p_1}{p_2}\frac{\partial x_1}{ \partial w}\right) x_1 \]
\[ = - \left( \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1} \right) - \left( \frac{p_1}{p_2}\frac{\partial x_1}{ \partial w}\right) \left( \frac{w - p_2 x_2}{p_1} \right) \]
\[ = - \left( \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1} \right) + \left( \frac{p_2 x_2 - w}{p_2}\frac{\partial x_1}{ \partial w}\right)\]
\[ = - \left( \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1} \right) - \frac{w}{p_2}\frac{\partial x_1}{ \partial w} + x_2 \frac{\partial x_1}{ \partial w} \]
Now, consider the homogeneity condition:
\[ x_1 (\alpha p_1, \alpha p_2, \alpha w) = x_1(p_1, p_2, w) \]
Differentiating both sides with respect to $\alpha$, we get
\[ \frac{\partial (\alpha p_1)}{\partial \alpha}\frac{\partial x_1 (\alpha p_1, \alpha p_2, \alpha w)}{\partial (\alpha p_1)}+\frac{\partial (\alpha p_2)}{\partial \alpha}\frac{\partial x_1 (\alpha p_1, \alpha p_2, \alpha w)}{\partial (\alpha p_2)}+\frac{\partial (\alpha w)}{\partial \alpha}\frac{\partial x_1 (\alpha p_1, \alpha p_2, \alpha w)}{\partial (\alpha w)} = 0 \]
\[ p_1 \frac{\partial x_1 ( p_1,  p_2,  w)}{\partial p_1}+p_2\frac{\partial x_1 ( p_1,  p_2, w)}{\partial p_2}+ w \frac{\partial x_1 ( p_1,  p_2,  w)}{\partial w} = 0 \]
\[ p_1 \frac{\partial x_1}{\partial p_1}+p_2\frac{\partial x_1}{\partial p_2}+ w \frac{\partial x_1}{\partial w} = 0 \]
\[ \frac{\partial x_1}{\partial p_2} = -\frac{p_1}{p_2} \frac{\partial x_1}{\partial p_1}-\frac{w}{p_2} \frac{\partial x_1}{\partial w}\]
Examining the work up until now, we have
\[ S_{21} =  - \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1}  - \frac{w}{p_2}\frac{\partial x_1}{ \partial w} + x_2 \frac{\partial x_1}{ \partial w} \]
Using the identity we derived from homogeneity, we get
\[ S_{21} =  - \frac{p_1 }{p_2}\frac{\partial x_1}{\partial p_1}  - \frac{w}{p_2}\frac{\partial x_1}{ \partial w} + x_2 \frac{\partial x_1}{ \partial w} \]
\[ = \frac{\partial x_1}{\partial p_2} +x_2 \frac{\partial x_1}{ \partial w} = S_{12}  \]
And hence for the two good case, the Slutsky matrix is symmetric.

Lastly, we provide an example of a 3-good scenario ``demand'' functions which are homogeneous of degree 0, satisfy Walras' law and WARP, but do not generate a symmetric Slutsky matrix.

Consider the demands:
\[ x_1 = w/p_2 \]
\[ x_2 = 2w/(3p_2) \]
\[ x_3 = \frac{w}{p_3}\left(1/3 - \frac{p_1}{p_2}\right) \]
conditional for this being an interior solution, we suppose that $p_1/p_2 < 1/3$ (so $x_3$ is nonnegative), and for WARP, we will suppose $p_3 < p_1$.

We can easily see that these demands are homogeneous of degree 0 and satisfy Walras' law. To check WARP, we know from problem set 2 that it suffices to check compensated price changes. We suppose that
\[ p_1 \frac{w'}{p_2'} + p_2 \frac{w'}{p_3'} + p_3 \frac{w'}{p_3'}\left(1/3 - \frac{p_1'}{p_2'}\right) = w \]
This implies
\[ \frac{p_1}{p_2'} + \frac{p_2}{p_3'} + \frac{p_3}{p_3'} \left(1/3 - \frac{p_1'}{p_2'}\right) = \frac{w}{w'}\]
\[ \frac{p_1}{p_2'} + \frac{p_2}{p_3'} + \frac{p_3}{3p_3'} - \frac{p_1'p_3}{p_2'p_3'}  = \frac{w}{w'}\]
\[ \frac{p_1p_3' + p_2p_2'+ p_3p_2'/3 - p_1'p_3}{p_2'p_3'}= \frac{w}{w'}\]
\[ \frac{w'}{w} = \frac{p_2'p_3'}{p_1p_3' + p_2p_2'+ p_3p_2'/3 - p_1'p_3}\]

Now, we note from our price conditions, because $p_1 > p_3$ and $p_1' > p_3'$, we must have
\[ p_1'p_1 (9p_1'p_1+9p_1p_3' +9p_1'p_3- 8p_3'p_3) > 0 \]
Further, because $p_2 > 2p_1$ and $p_2' > 2p_1'$, we get

\[ 2p_1p_3'p_1'p_3+p_1p_3'^2p_2/3-p_1^2p_3'^2  +p_2'^2p_2^2+p_2^2p_2'p_3'/3 + p_1'p_3^2p_2'/3+p_2'^2p_2p_3/3\]
\[ - 8p_3'p_2p_3p_2'/9-p_1p_3'p_3p_2'/3 - p_1'^2p_3^2 - p_1'p_3p_3'p_2/3 > p_1'p_1 (9p_1'p_1+9p_1p_3' +9p_1'p_3- p_3'p_3)\]

Combining these twe, we have
\[ 2p_1p_3'p_1'p_3+p_1p_3'^2p_2/3-p_1^2p_3'^2  +p_2'^2p_2^2+p_2^2p_2'p_3'/3 + p_1'p_3^2p_2'/3+p_2'^2p_2p_3/3\]
\[ - 8p_3'p_2p_3p_2'/9-p_1p_3'p_3p_2'/3 - p_1'^2p_3^2 - p_1'p_3p_3'p_2/3 > 0 \]

Upon close examination, we realize the LHS of this expression factors as
\[  (p_1p_3' + p_2p_2'+ p_3p_2'/3 - p_1'p_3) (p_1'p_3+p_2'p_2+p_3'p_2/3-p_1p_3') - p_2'p_3'p_2p_3 > 0  \]
Therefore
\[  (p_1p_3' + p_2p_2'+ p_3p_2'/3 - p_1'p_3) (p_1'p_3+p_2'p_2+p_3'p_2/3-p_1p_3') > p_2'p_3'p_2p_3   \]
\[\frac{p_2'p_3'}{p_1p_3' + p_2p_2'+ p_3p_2'/3 - p_1'p_3} < \frac{p_1'p_3+p_2'p_2+p_3'p_2/3 -p_1p_3'}{p_2p_3}  \]
Using the condition we found earlier, the LHS is exactly $w'/w$, so
\[\frac{w'}{w} < \frac{p_1'p_3+p_2'p_2+p_3'p_2/3-p_1p_3'}{p_2p_3}  \]
\[ \frac{p_1'}{p_2} + \frac{p_2'}{p_3} +  \frac{p_3'/3}{p_3} - \frac{p_1p_3'}{p_2p_3}  > \frac{w'}{w} \]
\[ \frac{p_1'}{p_2} + \frac{p_2'}{p_3} +  \frac{p_3'}{p_3}\left(1/3 - \frac{p_1}{p_2}\right) > \frac{w'}{w} \]
\[ p_1'\frac{w}{p_2} + p_2'\frac{w}{p_3} + p_3' \frac{w}{p_3}\left(1/3 - \frac{p_1}{p_2} \right) > w' \]
\[ p' \cdot x > w' \]
And hence WARP holds. Finally, we can observe the ``Slutsky'' matrix is not symmetric for these ``demands'':

\[\frac{\partial x_1}{\partial p_2} + \frac{\partial x_1}{\partial w}x_2 = -\frac{w}{p_2^2} + \frac{2w}{3p_2^2} = -\frac{w}{3p_2^2} \neq \frac{2w}{3p_2^2}  = \frac{\partial x_2}{\partial p_1} + \frac{\partial x_2}{\partial w}x_1 \]

Thus, Slutsky symmetry is not immediate for $L=3$.

\paragraph{(1.4)}
We note that these demand functions are already homogeneous of degree 0 in $p,w$, regardless of $\lambda$. In order to satisfy Walras's law, we must have
\[ \sum_{l=1}^L \lambda_lw = w \]
\[ \sum_{l=1}^L \lambda_l = 1 \]
This is clearly necessary, else Walras' law fails. To show sufficiency, we provide an expenditure function and utility function such that these are the demand functions of that utility function. To find the expenditure functions, we use Shepard's lemma:
\[ \frac{\lambda_l e(p, u)}{p_l} = \frac{\partial e(p,u)}{\partial p_l}  \]
\[ \frac{\lambda_l}{p_l} = \frac{1}{e(p,u)} \frac{\partial e(p,u)}{\partial p_l}  \]
\[ \lambda_l \log p_l + g(p_{-l}, u) = \log (e(p,u)) \]
Noting that this holds for all $l$, we have
\[ e(p,u) = g(u)\prod_{l=1}^L (p_l)^{\lambda_l}  \]
for some function $g$ of $u$. For simplicity, we can take the Cobb-Douglas:
\[ u(x) = \prod_{l=1}^L x_l^{\lambda_l} \]
and hence has the expenditure function:
\[ e(p,u) = u\frac{\prod_{l=1}^L (p_l)^{\lambda_l}}{\prod_{l=1}^L \lambda_l^{\lambda_l}} \]
\paragraph{(1.5)}
Suppose we have Hicksian demands $h_l$. We can calculate the expenditure function:
\[ e(p,u) = p \cdot h(p,u) \]
for some function $f$. Then the indirect utility satisfies:
\[ e(p,v(p,w)) = w   \]
\[ v(p,w) = e^{-1}\left( p, w \right) \]
where we invert $e$ in the second argument.
Then the Walrasian demands are
\[ x(p,w) = h(p,v(p,w)) = h\left(p, e^{-1}\left( p, w \right)\right) \]

On the other side, if we have Walrasian demands, we can use Shepard's lemma to generate a system of differential equations:
\[ \frac{\partial e}{\partial p_l} = x_l(p, e) \]
and solve for $e$. After this, we can again use Shepard's lemma to find $h$:
\[ h_l = \frac{\partial e}{\partial p_l} \]

For an example, we take Cobb-Douglas utility:
\[ u(x, y) = x^\alpha y^{1-\alpha} \]
Solving the EMP, we find the FOCs
\[ p_x = \lambda \alpha x^{\alpha - 1}y^{1- \alpha } \]
\[ p_y = \lambda (1-\alpha) x^{\alpha}y^{- \alpha } \]
\[ x^\alpha y^{1-\alpha} = u \]
Solving,
\[ h_x p_x = \lambda \alpha u \]
\[ h_y p_y = \lambda (1-\alpha) u \]
\[ h_x = \lambda \frac{\alpha u}{p_x} \]
\[ h_y = \lambda \frac{(1-\alpha) u}{p_y} \]
\[ \lambda \left(\frac{\alpha}{p_x}\right)^\alpha\left(\frac{1-\alpha}{p_y} \right)^{1-\alpha} = 1 \]
\[ \lambda =  \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha} \]
So our Hicksians are:
\[ h_x(p, u) = \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha} \frac{\alpha u}{p_x} \]
\[ h_y(p,u) = \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha}\frac{(1-\alpha) u}{p_y} \]
The expenditure function is then
\[ e(p,u) = \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha} u  \]
The indirect utility is then
\[ v(p,w) = w \left(\frac{\alpha}{p_x}\right)^\alpha\left(\frac{1-\alpha}{p_y} \right)^{1-\alpha} \]
Plugging these into the Hicksians, we get the expected Walrasian demands:
\[ x(p,w) = \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha} \frac{\alpha }{p_x}w \left(\frac{\alpha}{p_x}\right)^\alpha\left(\frac{1-\alpha}{p_y} \right)^{1-\alpha} = \frac{\alpha w}{p_x} \]
\[ y(p,w) = \left(\frac{p_x}{\alpha}\right)^\alpha\left(\frac{p_y}{1-\alpha} \right)^{1-\alpha} \frac{(1-\alpha) }{p_y}w \left(\frac{\alpha}{p_x}\right)^\alpha\left(\frac{1-\alpha}{p_y} \right)^{1-\alpha} = \frac{(1-\alpha) w}{p_y} \]
\paragraph{(1.6)}
\begin{itemize}
\item We note that since $x$ is feasible for prices $p, p\cdot x$, by utility maximization, we must have
\[ u(x) \le v(p, p\cdot x) \]
Since this holds for all $p$, we have
\[ u(x) \le \max_p v(p, p\cdot x) \]
Finally, it suffices to argue that for each $x$, $\exists p_x$ such that $u(x) = v(p, p\cdot x)$. This implies that $\max_p v(p, p\cdot) \ge u(x)$, and we will have the desired equality. Take $\tilde{p}$ as suggested in the problem, where
\[ \tilde{p}_l = \frac{\partial u}{\partial x_l} \] Consider $w = \tilde{p} \cdot x$. Consider $v(\tilde{p},w)$. We know that this is
\[ \max u(x)  \]
\[ \tilde{p} \cdot x \le w \]
Since $u$ is strictly increasing, quasiconcave, and differentiable, the solution satisfies the FOCs:
\[ \frac{\partial u}{\partial x_l} - \lambda \tilde{p}_l = 0 \]
\[ \tilde{p} \cdot x = w \]
By construction of $\tilde{p}_l$, we must have $\lambda = 1$. Further, since $x$ FOCs, we have that $x$ is a maximizer, and hence $v(\tilde{p}, \tilde{p}\cdot x) = u(x)$. And we are done.
\item The indirect utility function is homogeneous of degree 0 in $(p,w)$. Hence
\[ v(\lambda p, \lambda w) = (\lambda w)(\lambda p_1)^{-\alpha}(\lambda p_2)^{-\beta} = \lambda ^{1-\alpha - \beta} v(p,w) \]
In order to have the right homogeneity degree, we must have $1 - \alpha - \beta = 0$, or $\alpha + \beta = 1$.

We then use Roy's identity to find:
\[ x_1 = \frac{\alpha w p_1^{-\alpha - 1}p_2^{-\beta}}{p_1^{-\alpha}p_2^{-\beta}} = \frac{\alpha w}{p_1} \]
\[ x_2 = \frac{\beta w p_1^{-\alpha}p_2^{-\beta-1}}{p_1^{-\alpha}p_2^{-\beta}} = \frac{\beta w}{p_2} \]
Then we get
\[ u(x_1, x_2) = v(p,w) = \frac{w}{p_1^{\alpha}p_2^{\beta}} = \left(\frac{x_1}{\alpha}\right)^\alpha\left(\frac{x_2}{\beta}\right)^\beta  \]
\[ = \frac{x_1^{\alpha} x_2^{\beta}}{\alpha^\alpha \beta^\beta} \]
And this is the utility function. (We conveniently use the fact that we require $\alpha + \beta = 1$ in the middle of this.)
\end{itemize}
\pagebreak
\section*{Part 2}
\paragraph{(2.1)}
\begin{itemize}
\item We start by finding the FOCs:
\[ u'(x_1) + \lambda = 0 \]
\[ \delta(1+r) u'(x_2) + \lambda = 0 \]
\[(1+r)x_1 + x_2 - (1+r)I_1 - I
_2 = 0 \]
Then we have
\[ \begin{bmatrix}
u''(x_1) & 0 & 1 \\
0 &  \delta (1+r) u''(x_2) & 1 \\
1+r & 1 & 0
\end{bmatrix} \begin{bmatrix} (\partial x_1)/(\partial \delta) \\ (\partial x_2)/(\partial \delta) \\ (\partial \lambda)/(\partial \delta) \end{bmatrix} = \begin{bmatrix} 0 \\ -(1+r)u'(x_2) \\ 0 \end{bmatrix} \]
and

\[ \begin{bmatrix}
u''(x_1) & 0 & 1 \\
0 &  \delta (1+r)u''(x_2) & 1 \\
1+r & 1 & 0
\end{bmatrix} \begin{bmatrix} (\partial x_1)/(\partial r) \\ (\partial x_2)/(\partial r) \\ (\partial \lambda)/(\partial r) \end{bmatrix} = \begin{bmatrix} 0 \\ -\delta u'(x_2) \\ I_1-x_1 \end{bmatrix} \]
The bordered Hessian is:
\[H = \begin{bmatrix}
u''(x_1) & 0 & 1 \\
0 &  \delta(1+r) u''(x_2) & 1 \\
1+r & 1 & 0
\end{bmatrix} \]
We can compute the determinant:
\[ |H| = - u''(x_1)- \delta(1+r)^2 u''(x_2) \]
Note that we require $u'' < 0$ in order for the second order conditions to hold. Using Cramer's rule, we get
\[ \frac{\partial x_1}{\partial \delta} = \frac{\begin{vmatrix}
0 & 0 & 1 \\
-(1+r)u'(x_2) &  \delta (1+r)u''(x_2) & 1 \\
0 & 1 & 0 \end{vmatrix}}{- u''(x_1)- \delta (1+r)^2 u''(x_2)} = \frac{-u'(x_2)(1+r)}{- u''(x_1)- \delta (1+r)^2u''(x_2)} \]
\[ \frac{\partial x_2}{\partial \delta} = \frac{\begin{vmatrix}
u''(x_1) & 0 & 1 \\
0 & -(1+r)u'(x_2)  & 1 \\
1+r & 0 & 0 \end{vmatrix}}{- \frac{u''(x_1)}{(1+r)^2}- \delta u''(x_2)} = \frac{(1+r)^2u'(x_2)}{- u''(x_1)- \delta (1+r)^2u''(x_2)} \]
Assuming $u$ is concave and increasing, then we see that $\partial x_1 /\partial \delta$ is negative and $\partial x_2 / \partial \delta$ is positive. This makes intuitive sense; as the discount factor increases, people value their consumption in the future more, and hence will consume less now and save more.
\[ \frac{\partial x_1}{\partial r} = \frac{\begin{vmatrix}
0 & 0 & 1 \\
-\delta u'(x_2) &  \delta(1+r) u''(x_2) & 1 \\
I_1-x_1 & 1 & 0
\end{vmatrix}}{- u''(x_1)- \delta (1+r)^2u''(x_2)} = \frac{-\delta u'(x_2) + \delta (x_1 - I_1)(1+r) u''(x_2) }{- u''(x_1)- \delta (1+r)^2u''(x_2)} \]
\[ \frac{\partial x_2}{\partial r} = \frac{\begin{vmatrix}
u''(x_1) & 0 & 1 \\
0 &  -\delta u'(x_2) & 1 \\
1+r & I_1-x_1 & 0
\end{vmatrix}}{- u''(x_1)- \delta (1+r)^2u''(x_2)} = \frac{\delta (1+r) u'(x_2) +  x_1 u''(x_1) - I_1 u''(x_1)}{- u''(x_1)- \delta (1+r)^2u''(x_2)} = \frac{u'(x_1) +  (x_1-I_1) u''(x_1)}{- u''(x_1)- \delta (1+r)^2u''(x_2)}\]
These derivatives are slightly more nuanced to sign. If $x_1 < I_1$ (the consumer is saving in period 1), then clearly $x_2$ increases as $r$ increases, but it is not straightfoward what the sign of $\partial x_1 /\partial r$ is. However, if $x_1 \ge I_1$ (the consumer is borrowing in period 1), then $x_1$ decreases as $r$ increases, and it isn't exactly clear what $x_2$ does.
\item Let's pick $\tilde{u} = \log$. Then the utility function is Cobb-Douglas, so the demand functions are:
\[ x_1(\delta, r, I_1, I_2) = \frac{I_1 + \frac{I_2}{1+r}}{(1+\delta)} = \frac{(1+r)I_1 + I_2}{(1+\delta)(1+r)} \]
\[ x_2(\delta, r, I_1, I_2) = \delta \frac{I_1 + \frac{I_2}{1+r}}{(1+\delta)/(1+r)}= \delta \frac{(1+r) I_1 + I_2}{1 + \delta} \]
The relevant derivatives:
\[ \frac{\partial x_1}{\partial \delta} = - \frac{(1+r)I_1 + I_2}{(1+\delta)^2(1+r)} = -\frac{x_1}{1+\delta}\]
\[ \frac{\partial x_2}{\partial \delta} = \frac{(1+r) I_1 + I_2}{1 + \delta} - \delta \frac{(1+r) I_1 + I_2}{(1 + \delta)^2} \]
\[ = \frac{(1+r) I_1 + I_2}{(1 + \delta)^2} \]
\[ \frac{\partial x_1}{\partial r} = \frac{ - I_2}{(1+\delta)(1+r)^2} \]
\[ \frac{\partial x_2}{\partial r} = \frac{\delta I_1}{1+\delta} \]
\item We see from our derivatives that as $r$ increases, consumers will consume less in period $1$ since $\partial x_1 / \partial r < 0$, and hence will save more. Consumers then consume more in period $2$, since $\partial x_2 / \partial r > 0$. The larger $I_2$ is, the higher the rate at which people will save money in period $1$ as interest rates rise. The larger $I_1$ is, the higher the rate of consumption will increase in period 2 as a result of an interest rate increase.
\end{itemize}
\paragraph{(2.2)}
\begin{itemize}
\item We generally suppose $\delta < 1$, implying that the consumer is myopic and discounts future payoffs in their present decision-making. However, after time actually progresses, the users may ``regret'' previous choices as their future utility function changes to $\tilde{u}(x_2)$. Hence, it is reasonable to reason that policy might want to actually maximize $\tilde{u}(x_1) + \tilde{u}(x_2)$ rather than the myopic time 1 utility $\tilde{u}(x_1) + \delta \tilde{u}(x_2)$ in order to minimize future regret. Comparing these two objectives, we see that an optimal bundle for the first objective requires a higher $x_2$ than the optimal bundle under the second, and hence it makes sense to want to increase savings.

Mathematically speaking, consider
\[ \frac{\partial u}{\partial x_2} = \tilde{u}'(x_1) \frac{\partial x_1}{\partial x_2} + \delta \tilde{u}'(x_2) \]
Now from the budget constraint,
\[ \frac{\partial x_1}{\partial x_2} = \frac{-1}{1+r} \]
So
\[ \frac{\partial u}{\partial x_2} = \delta \tilde{u}'(x_2) -  \frac{\tilde{u}'(x_1)}{1+r}  \]
This is positive when
\[ \delta \tilde{u}'(x_2)  >  \frac{\tilde{u}'(x_1)}{1+r} \]
\[ \delta(1+r) \tilde{u}'(x_2) > \tilde{u}'(x_1) \]
Hence it is possible that this condition fails; that as $x_2$ increases, utility actually decreases. However, we are generally interested in cases where where $I_1 >> I_2$, since the assumption is that consumers have significantly less income in retirement. In this scenario, increasing interest rates expands the budget set, affording users higher utility. Thus, although it is mathematically possible for increasing savings to decrease utility, we generally consider the condition for which in our model the opposite is true, specifically for changes in $r$.
\item The model described above is relevant for a myopic consumer (which perhaps people are). For example, by expanding the time domain, where the consumer makes decisions over a higher frequency, the generalized form that we might argue is ``better'' might be to take utiltiy as $\sum \delta^t \tilde{x_t}$ and maximize consumption over a lifetime budget constraint over many different interest rates:
\[ \sum \frac{x_t}{(1+r_t)^t} =  \sum \frac{I_t}{(1+r_t)^t} \]
Now, if $\delta$ is small, then to first-order approximation in $\delta$, the generalized problem becomes exactly the problem in 2.1. Hence, our solutions can be relevant for small $\delta$, for one case. For another take, we can rewrite the budget constraint as
\[ (1+r)x_1 + x_2 = (1+r)I_1 + I_2 \]
We can reinterpret this as a sequential equilibrium constraint rather than an Arrow-Debreu equilibrium constraint, where the optimization choice is over only two time periods. Hence, the comparative statics might still be relevant for considering the short term sequential equilibrium.
\end{itemize}
\paragraph{(2.3)}
\begin{itemize}
\item We convert this into:
\[ \max \pi \tilde{u}(x_1) + (1-\pi) \tilde{u}(w - x_1) = \max f(x_1, \pi) \]
We can quickly observe that, assuming $\tilde{u}$ is differentiable and strictly increasing,
\[ \frac{\partial^2 }{\partial x_1 \partial \pi} = \tilde{u}'(x_1) + \tilde{u}'(w-x_1) > 0 \]
We then know that $f$ satisfies strictly increasing differences. Then we know from monotone comparative statics, that as $\pi$ increases, $x_1$ also increases.
\item With multiple goods, we have
\[ \max \pi_1 \tilde{u}(x_1) + \pi_2 \tilde{u}(x_2) .... + (1 - \sum_{i=1}^{n-1} \pi_i )\tilde{u} \left(w - \sum_{i=1}^{n-1} x_i \right) = f(\vec{x}, \vec{\pi}) \]
Once again, supposing $\tilde{u}$ is differentiable and strictly increasing, we can easily confirm that for all $i$,
\[ \frac{\partial^2 }{\partial x_i \partial \pi_i} = \tilde{u}'(x_i) + \tilde{u}'\left(w - \sum_{i=1}^{n-1} x_i \right) > 0  \]
and further, for $i, j$, where $j \neq i$,
\[ \frac{\partial^2 }{\partial x_i \partial \pi_j} = \tilde{u}'\left(w - \sum_{i=1}^{n-1} x_i \right) > 0 \]
Hence, we have established that the maximization function $f$ is supermodular so once again we can apply monotone comparative statics; as $\pi_i$ increases so does $x_i$.
\end{itemize}
\paragraph{(2.4)}
For the form given, we are interested in comparative statics for $\partial x_l / \partial p_1$. (We can establish related conditions for $p_i$ by substituting out $x_i$ using the budget constraint). In order for us to be able to use monotone comparative statics (proposition 8), we need increasing differences in every pair $(x_l, p_1)$, $(x_l, w)$, and $(x_l, x_k)$.

%That is, it suffices to have supermodularity in $x, p_1, w$. Suppose we have $(x, p_1, w)$ and $(x', p_1', w')$. Take $\min(x_l, x_l') = \underline{x_l}$ and $\max(x_l, x_l') = \overline{x_l}$ (with similar notation for $p_1, w$). Then the supermodularity condition is, for $x, x'$:
%\[ u\left(\frac{1}{p_1}\left( w - \sum_{l=2}^L p_l x_l \right), x_2, ... x_L \right) + u\left(\frac{1}{p_1'}\left( w' - \sum_{l=2}^L p_l x_l' \right), x_2', ... x_L' \right)\]\[ \le u\left(\frac{1}{\underline{p_1}}\left( \underline{w} - \sum_{l=2}^L p_l \underline{x_l} \right), \underline{x_2}, ... \underline{x_L}\right) + u\left(\frac{1}{\overline{p_1}}\left( \overline{w} - \sum_{l=2}^L p_l \overline{x_l} \right), \overline{x_2}, ... \overline{x_L}\right) \]
Then by proposition 8, the maximizer values $x_l^*$ will be nondecreasing in $p_1$, $w$.

We exhibit a function form of $u$ that satisfies this condition:

TODO finish

\paragraph{(2.5)}

TODO idfk 

\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
